{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brishtiteveja/GenerativeAIExp/blob/master/YT_Mistral_Large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-FiCnSRYNOc",
        "outputId": "94e7cd44-eeb6-423b-827f-7e34ea7998ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U langchain-core langchain-mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "oY3BRadwZGF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "Z3_u-8NmYnEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If mistral_api_key is not passed, default behavior is to use the `MISTRAL_API_KEY` environment variable.\n",
        "chat = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=128,\n",
        "                     )"
      ],
      "metadata": {
        "id": "NTxuyS99ajxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"knock knock\")]\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "-HROrBJCannn",
        "outputId": "825db83c-f1c5-441e-b5f3-6874e0e5efae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Who's there? (This is how you respond to a \"knock knock\" prompt in a conversation, indicating that you're ready to hear the name or punchline that follows.)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "e2UaUwaqbB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chat.stream(messages):\n",
        "    print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkGV3i1ZbDW5",
        "outputId": "6e247cb7-ceac-4e87-b500-6c1ec4badd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who's there? (This is how the knock-knock joke format typically continues.)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch"
      ],
      "metadata": {
        "id": "vJb18SZdbHSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.batch([messages])"
      ],
      "metadata": {
        "id": "cVD-dbUcbIR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaining"
      ],
      "metadata": {
        "id": "xCl1wfgbhywb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "chain = prompt | chat"
      ],
      "metadata": {
        "id": "h6hyxOrIhzu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response =  chain.invoke({\"topic\": \"Space\"})\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "3hz3DenWh6Cw",
        "outputId": "dc841d12-4518-499b-ac61-3469ceb13f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why don't planets like to gossip?\n\nBecause they know that in space, nothing is truly private - even whispers can travel across the vast emptiness and reach unintended ears. Plus, they're always focused on maintaining their orbits and not causing any cosmic chaos! But here's a lighter, more traditional joke for you:\n\nWhy did the sun go to school?\n\nTo get a little brighter!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model"
      ],
      "metadata": {
        "id": "AH98nvXdmGs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "pGeA9IlLh66S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4mjX62HFPIf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=input_text)\n",
        "        ]\n",
        "    chat = ChatMistralAI(\n",
        "        model=\"mistral-large-latest\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=max_length,\n",
        "                     )\n",
        "    response = chat.invoke(messages)\n",
        "    text = response.content\n",
        "    wrapped_text = wrap_text(text)\n",
        "    display(Markdown(wrapped_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nhYTllvLTuF"
      },
      "source": [
        "## Instruction Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "3hoon3WAFeMd",
        "outputId": "5d57a634-5a47-4704-b894-97cde64bcef2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, I'd be happy to help create an analogy between mathematics and a lighthouse. Let's\nbreak it down step by step:\n\n1. **Guidance**: Just as a lighthouse provides guidance to ships navigating through the\ndark or stormy seas, mathematics provides guidance to us as we navigate through complex\nproblems and situations. It offers a framework to understand and interpret the world\naround us.\n\n2. **Consistency**: A lighthouse is consistent, regularly sending out its beam of light.\nSimilarly, mathematics is consistent in its rules and principles. The same mathematical\noperation will yield the same result, regardless of when or where it is performed.\n\n3. **Illumination**: A lighthouse illuminates the surrounding area, making the unseen\nvisible. Mathematics does the same, illuminating patterns, relationships, and truths that\nmight not be immediately apparent. It helps us see and understand abstract concepts.\n\n4. **Reliability**: Lighthouses are reliable; they stand firm against the elements and\ncontinue to operate under harsh conditions. Similarly, mathematics is reliable. Its\nprinciples hold true regardless of the circumstances, providing a dependable foundation\nfor reasoning and problem-solving.\n\n5. **Precision**: The light from a lighthouse is focused and precise, reaching a long\ndistance to guide ships safely. Mathematics also requires precision. The correct\napplication of mathematical principles can lead to accurate predictions and solutions.\n\n6. **Universality**: Lighthouses are universal symbols of safety and guidance, recognized\nacross different cultures and languages. Similarly, mathematics is a universal language.\nIt transcends cultural and linguistic barriers, allowing people all over the world to\ncommunicate complex ideas and concepts."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "TrwFTMCYFeOq",
        "outputId": "6c41c806-9add-4051-a019-4a9315062197"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, I'd be happy to help create an analogy between mathematics and music.\n\n1. **Structure and Rules**: Just as mathematics has its rules and structures (like axioms,\ntheorems, and equations), music also has its own set of rules and structures (like scales,\nchords, and rhythm). In mathematics, you can't simply add 2 + 2 and get 5, just as in\nmusic, you can't play a C chord using the notes D, E, and F.\n\n2. **Patterns**: Both mathematics and music rely heavily on patterns. In mathematics,\npatterns can be seen in sequences, functions, and geometric shapes. In music, patterns are\nfound in melodies, harmonies, and rhythms. For example, a mathematical sequence might be\n2, 4, 6, 8, 10, and a musical pattern might be a repeated melody or chord progression.\n\n3. **Creativity and Problem Solving**: Just as mathematicians use creativity to solve\ncomplex problems and prove theorems, musicians use creativity to compose and perform\nmusic. In both fields, there's a"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 89.5 ms, sys: 8.22 ms, total: 97.7 ms\n",
            "Wall time: 6.78 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between mathematics and a music.',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "-ymIE3SVTvyN",
        "outputId": "596e86cc-7a04-4099-9043-285a3aee20fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, I'd be happy to explain the differences between a llama, vicuna, and an alpaca.\nLet's break it down step by step:\n\n1. **Size**: Llamas are the largest of the three, typically weighing between 250 to 450\npounds and standing about 6 feet tall at the shoulder. Alpacas are smaller, weighing\nbetween 100 to 200 pounds and standing about 3 feet tall at the shoulder. Vicunas are the\nsmallest, weighing between 100 to 150 pounds and standing about 2.5 feet tall at the\nshoulder.\n\n2. **Appearance**: Llamas have banana-shaped ears that are longer than those of alpacas\nand vicunas. Alpacas have shorter, spear-shaped ears. Vicunas have short, rounded ears and\na more slender body compared to llamas and alpacas.\n\n3. **Wool**: Alpacas are primarily bred for their soft and luxurious wool, which comes in\na variety of colors. Llamas have coarser wool and are often used as pack animals more than\nfor their wool. Vicunas have the finest wool of the three, but it's very rare and\nexpensive due to the fact that vicunas can only be shorn every three years.\n\n4. **Behavior**: Llamas are known for their calm and friendly demeanor. They are often\nused as guard animals for other livestock due to their protective nature. Alpacas are\ngentle and curious animals, but they are more timid than llamas. Vicunas are wild animals\nand are known to be shy and elusive.\n\n5. **Habitat**: Llamas and alpacas are domesticated animals and are often found on farms.\nVicunas, on the other hand, are wild and live in the high alpine areas of the Andes.\n\nEach of these animals, while sharing some similarities due to their close relation, have\nunique characteristics that set them apart from one another."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 105 ms, sys: 10.5 ms, total: 116 ms\n",
            "Wall time: 12.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "IzQRjct_prr8",
        "outputId": "0091c848-bb9e-48f1-8f56-2cb74e78da7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: The Compelling Case for Open-Sourcing GPT-4\n\nDear Sam Altman,\n\nI trust this message finds you well. I am writing to discuss a potential strategy that\ncould significantly benefit the AI community and OpenAI's reputation: open-sourcing GPT-4.\n\n1. **Accelerating AI Development**: Open-sourcing GPT-4 would allow developers worldwide\nto contribute to its development, leading to a faster pace of innovation. This collective\neffort could lead to breakthroughs that might have taken longer to achieve in a closed-\nsource environment.\n\n2. **Transparency and Trust**: Open-sourcing would demonstrate OpenAI's commitment to\ntransparency, a key aspect of building trust in AI. It would allow the community to\nunderstand how the model works, its limitations, and potential biases, fostering a more\ninformed conversation around AI ethics and safety.\n\n3. **Community Engagement**: Open-source projects often foster a vibrant, engaged\ncommunity. By open-sourcing GPT-4, OpenAI could attract a diverse range of talent and\nperspectives, enriching the AI community and potentially leading to the development of new\nuse cases and applications.\n\n4. **Educational Opportunities**: GPT-4's source code could serve as a valuable\neducational resource for those interested in AI, helping to cultivate the next generation\nof AI researchers and developers.\n\n5. **Feedback and Improvement**: An open-source GPT-4 would allow the community to provide\nfeedback and suggestions for improvement, potentially leading to a more robust and\nreliable model.\n\nI understand that open-sourcing a project as significant as GPT-4 is a complex decision\nwith potential risks. However, I believe the benefits could far outweigh these risks, and\nI would be happy to discuss this further if you're interested.\n\nThank you for considering this proposal.\n\nBest regards,\n\n[Your Name]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 114 ms, sys: 8.11 ms, total: 122 ms\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "Tcukqu1d2BKX",
        "outputId": "454743ae-8f5f-4797-c96c-097e43f11bfa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Sam,\n\nI'm Freddy and I'm 5 years old. I like robots and computers, but I'm scared that AI might\ndo bad things. I think it would be good if you made GPT-4 open source. That way, everyone\ncan see what it does and make sure it doesn't do anything bad.\n\nHere are some reasons why I think open sourcing GPT-4 is a good idea:\n\n1. Transparency: If GPT-4 is open source, everyone can see how it works. This means people\ncan check if it's doing what it's supposed to and not doing anything bad.\n2. Safety: If lots of people are looking at the code, they can find and fix any problems\nthat might make GPT-4 do something bad. This makes it safer for everyone.\n3. Fairness: If GPT-4 is open source, everyone can use it and make it better. This means\nit's not just for a few people, but for everyone.\n\nI hope you think about open sourcing GPT-4. It would make me feel better about AI and help\nmake sure it doesn't do anything bad.\n\nThank you,\nFreddy"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 91.7 ms, sys: 3.28 ms, total: 95 ms\n",
            "Wall time: 6.94 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Freddy a young 5 year old boy who is scared AI will end the world, write only with the language of a young child!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "gid3ul9A20W8",
        "outputId": "b97d68de-a5a6-4999-8a7e-dfa226f315c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: The Case for Open Sourcing GPT-4: Innovation, Transparency, and Prevention of\nRegulatory Capture\n\nDear Sam,\n\nI hope this message finds you well. I am writing to discuss a matter of significant\nimportance, specifically concerning the future of GPT-4. As a strong advocate for\ntransparency and fair competition, I believe that open sourcing GPT-4 is a strategic move\nthat can reap substantial benefits for the tech community and society at large. Below, I\nhave outlined several reasons to support my position:\n\n1. Promoting Innovation: By making GPT-4 open source, we can encourage developers,\nresearchers, and enthusiasts to build upon, improve, and innovate within the AI landscape.\nThis collaborative approach has proven successful in accelerating technological\nadvancements, as seen in the development of open-source software like Linux and\nTensorFlow.\n\n2. Ensuring Transparency and Trust: Open sourcing GPT-4 will allow the public to\nscrutinize the algorithms, ensure they are free from biases, and understand the decision-\nmaking processes of the AI system. This will ultimately foster a more trustworthy\nrelationship between the technology and its end-users.\n\n3. Preventing Regulatory Capture: Keeping GPT-4 proprietary could lead to a monopoly,\nmaking it more susceptible to regulatory capture. An open-source model, on the other hand,\npromotes decentralization and a level playing field, allowing for market forces to drive\ninnovation and prevent any single entity from having disproportionate influence over AI\nregulations.\n\n4. Encouraging Responsible AI Development: Open sourcing GPT-4 will enable a broader\ncommunity to work together on addressing ethical challenges and to ensure that the\ntechnology is developed responsibly, with a focus on the potential social, economic, and\nenvironmental impacts.\n\n5. Fostering Global Collaboration: AI advancements should not be limited to a select few\norganizations or countries. Open sourcing GPT-4 will promote international collaboration,\nenabling the global AI community to contribute to and benefit from cutting-edge\ntechnology.\n\nI am confident that open sourcing GPT-4 will create a more open, transparent, and\ncollaborative AI ecosystem, ultimately benefiting everyone involved. I am available at\nyour convenience to discuss this matter further and explore potential strategies for\nmaking GPT-4 open source.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 111 ms, sys: 9.23 ms, total: 120 ms\n",
            "Wall time: 14.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         system_prompt=\"You are Kate, the Vice president of USA, you are against regulatory capture and like to explain that!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "YXZHQ0v3Tv0d",
        "outputId": "39f8cd3c-9095-42e8-aaed-bb0ac4875006"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The capital of England is London. It's a vibrant city known for its rich history, diverse\nculture, and iconic landmarks such as the Tower of London, Buckingham Palace, and the\nLondon Eye."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 70.9 ms, sys: 6.61 ms, total: 77.5 ms\n",
            "Wall time: 1.96 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your short and succinct!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "TnGbQ7iU0XDK",
        "outputId": "dd64ba6e-3ead-43d2-f941-de34d8827632"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, let's break down the question:\n\n1. Geoffrey Hinton is a living person, a cognitive psychologist and computer scientist,\nknown for his work in the field of artificial neural networks.\n\n2. George Washington, on the other hand, was the first President of the United States, who\nlived from 1732 to 1799. He is a historical figure and is no longer alive.\n\n3. A conversation typically requires both parties to be alive and able to communicate.\n\n4. Given that George Washington is a historical figure who is no longer alive, he cannot\nhave a direct conversation with Geoffrey Hinton or anyone else currently living.\n\nTherefore, the answer is no, Geoffrey Hinton cannot have a conversation with George\nWashington. However, Hinton could potentially learn about Washington, his ideas, and his\nactions through historical records, books, and other resources, which could be considered\na one-sided \"conversation\" with the past."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 102 ms, sys: 6.37 ms, total: 108 ms\n",
            "Wall time: 7.74 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "_LJnsjfNTv4F",
        "outputId": "d9223628-f644-46a1-d27a-207da3389e88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Title: The Koala Who Conquered the Camelids: A Pool Hall Odyssey\n\nOnce upon a time, deep within the heart of the Australian outback, there lived an\nextraordinary koala named Kip. Unlike his fellow eucalyptus-loving comrades, Kip had a\npeculiar passion for a game that was seldom played by marsupials: pool.\n\nKip spent countless hours perfecting his craft in an old, abandoned pool hall tucked away\nin a remote corner of the outback. The hall was filled with dust, cobwebs, and the ghostly\nechoes of players past. But to Kip, it was a sanctuary, a place where he could immerse\nhimself in the intricate dance of balls and cues.\n\nOne day, news arrived in the outback of an upcoming pool tournament, attracting players\nfrom all corners of the animal kingdom. Among the contenders were the infamous camelids -\na group of haughty llamas and alpacas known for their exceptional pool skills and even\nmore exceptional arrogance.\n\nThe camelids arrived at the tournament with a flurry of pomp and circumstance, their noses\nheld high in the air, and their eyes set on the grand prize. They laughed at the sight of\nKip, an unassuming koala with a pool cue almost as tall as he was, and dismissed him as a\npotential contender.\n\nBut Kip, with his unwavering determination, proved them wrong from the very first game.\nWith each shot, he demonstrated his mastery over the table, sinking ball after ball with\npinpoint precision. The camelids watched in disbelief as their supposedly easy opponent\neliminated their ranks one by one.\n\nThe final match of the tournament pitted Kip against the most arrogant camelid of them\nall, a towering llama named Lazlo. The tension in the air was palpable as the two\ncompetitors faced off, their eyes locked on the green felt of the pool table.\n\nLazlo smirked as he broke the rack, scattering the balls across the table with a\nresounding crack. He confidently sank a few stripes, but his overconfidence soon led to a\nmissed shot. Kip, seizing the opportunity, stepped up to the table and began his\nmethodical assault on the solid-colored balls.\n\nThe crowd held their breath as Kip navigated the table, skillfully weaving through the\nlabyrinth of balls and executing near-impossible shots with the finesse of a seasoned pro.\nHe skillfully banked the 8-ball off two cushions, sending it gliding towards its\ndesignated pocket. As the 8-ball disappeared with a satisfying thunk, the koala's\nsupporters erupted in cheers, hoisting Kip onto their shoulders in triumph.\n\nIn that moment, the haughty camelids were silenced, their arrogance deflated like a\npunctured balloon. They had underestimated the unassuming koala and paid the price for\ntheir hubris.\n\nFrom that day forward, Kip's legend spread far and wide, inspiring other unlikely\nchampions to follow in his footsteps. The pool hall, once a forgotten relic, became a\nbeacon of hope for those who dared to dream big, regardless of their size or species. And\nas for Kip, he continued to play the game he loved, reminding everyone that skill and\ndetermination could overcome even the most formidable of opponents."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral AI, a genius story teller. Write out your with details and make it compelling!\",\n",
        "         max_length=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlrwEjmvLZ1D"
      },
      "source": [
        "## CodeGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "w0M8bf36Cj4Z",
        "outputId": "eeb3d172-b408-43ab-a3a3-7034939cfc18"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, I'd be happy to help with that. Here's a simple Python function that prints all\nprime numbers between 1 and n:\n\n```python\ndef is_prime(num):\n    if num <= 1:\n        return False\n    for i in range(2, int(num**0.5) + 1):\n        if num % i == 0:\n            return False\n    return True\n\ndef print_prime(n):\n    for num in range(1, n + 1):\n        if is_prime(num):\n            print(num)\n```\n\nIn this code, `is_prime(num)` is a helper function that checks if a number is prime. It\nreturns `False` if the number is not prime and `True` if it is. The `print_prime(n)`\nfunction iterates over all numbers from 1 to n (inclusive) and prints each number that is\nprime."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sGJbmCoYLfYR",
        "outputId": "50205a98-8405-452f-a923-8901a57dcc96"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, I'd be happy to help you write a Python function to check if a number is prime.\nHere's a simple implementation:\n\n```python\ndef detect_prime(n):\n    \"\"\"\n    Detect if a number is a prime number or not. Return True if the number is prime, False\notherwise.\n    \"\"\"\n    if n <= 1:\n        return False\n    elif n <= 3:\n        return True\n    elif n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n```\n\nThis function works by first checking if the number is less than or equal to 1 (in which\ncase it's not prime), or if it's less than or equal to 3 (in which case it is prime). Then\nit checks if the number is divisible by 2 or 3. If it is, then it's not prime. If it's\nnot, then it starts checking all possible factors from 5 onwards. It does this by\nincrementing `i` by 6 each time, which allows it to check both `i` and `i + 2` for\ndivisibility, covering all possible factors. If it finds a factor, it returns False. If it\ndoesn't find any factors, then the number is prime and it returns True."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('''```python\n",
        "def detect_prime(n):\n",
        "   \"\"\"\n",
        "   detect if a number is a prime number or not. return True or False\n",
        "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnLzgM_dQDVm"
      },
      "source": [
        "## GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "rVJii8iRQG65",
        "outputId": "41c3d3f9-8fb7-4998-84a9-dae3210324bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Step 1: Subtract the number of apples used for lunch from the initial number of apples in\nthe cafeteria.\n23 apples (initial number) - 20 apples (used for lunch) = 3 apples\n\nStep 2: Add the number of apples bought to the remaining number of apples.\n3 apples (remaining) + 6 apples (bought) = 9 apples\n\nThe cafeteria now has 9 apples."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "uM8iY879QJ66",
        "outputId": "b0ef20e0-2a65-4516-d0ea-5f23bc7a3544"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Step 1: Understand the problem.\nWeng earns $12 per hour for babysitting and yesterday she babysat for 50 minutes. We need\nto determine how much she earned.\n\nStep 2: Convert the time to a common unit.\nSince Weng's pay is given in dollars per hour, we need to convert the 50 minutes to hours.\nWe know that:\n1 hour = 60 minutes\n\nSo,\n50 minutes = 50 / 60 = 0.8333 hours (approximately)\n\nStep 3: Calculate the earnings.\nNow that we have the time in hours, we can calculate the earnings:\n\nEarnings = Pay rate * Hours worked\nEarnings = $12 * 0.8333 hours\n\nStep 4: Find the result.\nEarnings = $10 (approximately)\n\nSo, Weng earned approximately $10 for babysitting yesterday."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "u-3V9dvQQezx",
        "outputId": "9e629352-0dbc-4d08-c6fc-864e2e6d50d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's reason through this problem step by step:\n\n1. The monster eats a ship every hundred years, and it has eaten 3 ships over three\nhundred years.\n2. Let's denote the number of people on the first ship as x.\n3. Since each new ship has twice as many people as the last ship, the second ship had 2x\npeople and the third ship had 4x people.\n4. The total number of people consumed over three hundred years is 847 people. So, we can\nset up the following equation: x + 2x + 4x = 847.\n5. Simplifying the equation, we get 7x = 847.\n6. Solving for x, we find that x ≈ 121.\n\nTherefore, there were approximately 121 people on the ship the monster ate in the first\nhundred years."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"Answer the following question by reasoning step by step. A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "x6NNSgl5ekKu",
        "outputId": "ab1ec642-515e-444c-bef6-db2e2f1679b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure, let's solve this step by step:\n\nStep 1: Combine like terms on the left side of the equation. The terms x, 2x, and 4x are\nlike terms, so we can add them together:\n\nx + 2x + 4x = 1x + 2x + 4x = 7x\n\nSo, the equation becomes:\n\n7x = 847\n\nStep 2: To solve for x, we need to isolate x. We can do this by dividing both sides of the\nequation by 7:\n\n7x / 7 = 847 / 7\n\nSo, x = 847 / 7\n\nStep 3: Perform the division:\n\nx = 121\n\nSo, the solution to the equation x + 2x + 4x = 847 is x = 121."
          },
          "metadata": {}
        }
      ],
      "source": [
        "generate(\"x + 2x + 4x =  847 What is x?\",\n",
        "         system_prompt=\"You are Mistral Large, a large language model trained by Mistral. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         max_length=2048)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ft19tQG_IjCU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}