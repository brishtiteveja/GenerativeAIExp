{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brishtiteveja/GenerativeAIExp/blob/master/YT_Llama_3_1_70b_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAV2vR8fNLH9",
        "outputId": "420f771e-a298-40db-a91d-2151c3a590fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "ddjh7AZ1NjEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "_2hJx_G3NrQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf-xqJU9NYxe",
        "outputId": "76d858e5-0431-4a2b-a88d-a6354898d027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models have revolutionized the way we interact with technology and access information. Their importance can be understood from several perspectives, encompassing both the direct benefits to users and the broader societal implications.\n",
            "\n",
            "1. **Accessibility and Speed**: The most direct advantage of fast language models is the rapid processing of information they provide. They enable immediate answers or feedback in a multitude of tasks such as searching for information online, drafting documents, providing real-time language translation, and interacting with customer service interfaces, thereby making life faster and more convenient for everyone.\n",
            "\n",
            "2. **Economic Benefits**: Businesses also significantly benefit from the deployment of fast language models. In sectors such as customer service, quick language models allow companies to automatically manage customer queries 24/7 without a large labor force. This leads to significant savings and improvements in service availability, round-the-clock assistance without significant incremental cost. This cost reduction in manual support combined with continuous accessibility drives better customer experiences, thus having economic implications in increasing satisfaction, improving brands’ goodwill, and overall consumer relationship development.\n",
            "\n",
            "3. **Knowledge Dissemination and Democratization of Access to Information**: These models aid in overcoming linguistic and informational barriers by rapidly generating texts that cater to any number of tasks across language mediums such as documents writing tools such as re-writing (similar tool set uses; email responder help suggestions examples assist real-use functionality):  \n",
            "\t- Educational environments (individual understanding guidance toward providing options individual topic base discussions analysis perspectives opinions within written synthesis techniques guided insight enable proactive scholarly exercise developments all age and comprehension; and  \n",
            "\t- Quick summarization capabilities make complex information easily consumable by a wider audience, democratizing access to and understanding of advanced information. In education, this could enable students from lower income backgrounds or developing countries to have quality detailed lesson guidance materials with structured and guided learning journeys despite local limits such as teacher inadequacy.\n",
            "\n",
            "4. **Technological Advancements and Further Research**: The reliance and heavy investment on fast language models push further research in this area. This not only enhances their capabilities to perform more tasks with higher efficiency but also invites breakthroughs in related technological and scientific fields, such as machine learning and cognitive computing. Improved language processing also provides insights into human minds and language structures, furthering linguistic research.\n",
            "\n",
            "5. **Enhancing Productivity and Work Automation**: With the ability to perform a wide range of tasks such as writing, proofreading, and data query answering in real-time, fast language models increase the productivity of human labor by automating routine activities. This allows individuals to focus on higher-level, more complex tasks that add more value, thereby driving innovation and competitiveness in the global economy.\n",
            "\n",
            "In summary, the importance of fast language models is inherent in their ability to provide immediate and efficient language processing, enhance business operations, democratize access to information, drive technological innovation, and augment human productivity across multiple sectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e__gEgRuN0Xn",
        "outputId": "b06ae789-b8cd-4361-afb2-9ac3d79e8c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models, also known as efficient or compact language models, have become increasingly important in recent years due to several advancements in the field of natural language processing (NLP). Here are some key reasons why fast language models have gained significance:\n",
            "\n",
            "1. **Scalability**: Fast language models can process large texts quickly and efficiently, making them suitable for applications that require massive text analysis, such as search engines, chatbots, and content analysis platforms.\n",
            "2. **Memory efficiency**: Large language models require significant memory to store and process. Fast language models can fit into memory storage, allowing for more models to be maintained without significant increases in memory consumption.\n",
            "3. **Quick inference times**: Fast language models enable faster inference times, which is critical in applications that require rapid response times, such as:\n",
            "\t* Chatbots and virtual assistants that need to respond promptly to user queries.\n",
            "\t* Text analysis and sentiment analysis applications that need to quickly compute large datasets.\n",
            "\t* Content generation tools that need to produce text rapidly.\n",
            "4. **Real-time processing**: With fast language models, applications can perform real-time text processing, enabling use cases such as:\n",
            "\t* Live chat and sentiment analysis in customer service.\n",
            "\t* Real-time content moderation and detection.\n",
            "\t* Streaming text analysis and monitoring.\n",
            "5. **Edge AI**: As more devices become connected, fast language models enable the possibility of edge AI, where AI processing can occur on the device itself (e.g., smartphone, smart speaker), reducing latency and improving overall user experience.\n",
            "6. **Adversarial robustness**: Fast language models can be more resistant to adversarial attacks, which attempt to manipulate the model's predictions by generating specific input patterns. By being able to quickly process and evaluate input, fast language models can better detect and mitigate such attacks.\n",
            "7. **Regularization techniques**: Fast language models often employ regularization techniques, such as pruning and knowledge distillation, which can improve model performance and robustness while reducing computational requirements.\n",
            "8. **Multitask learning**: Fast language models can be fine-tuned for various NLP tasks, enabling multitask learning, where a single model can learn to perform multiple related tasks simultaneously, such as language translation, text classification, and sentiment analysis.\n",
            "9. **Device-agnostic deployment**: Fast language models can be deployed on various devices, including those with limited computational resources, such as mobile devices and embedded systems.\n",
            "10. **Ensemble learning**: Fast language models can be combined with other models to create ensemble models, which can leverage their strengths and improve overall performance on specific tasks.\n",
            "\n",
            "Some notable examples of fast language models include:\n",
            "\n",
            "* Distilled BERT (distilBERT)\n",
            "* SlimBERT\n",
            "* MobileBERT\n",
            "* Albert\n",
            "* FlauBERT\n",
            "* FusionBERT\n",
            "* Knowledge Distilled BERT (K-DistilBERT)\n",
            "\n",
            "These fast language models have been applied in various domains, including language translation, text classification, sentiment analysis, question answering, and natural language generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(query, system_prompt=\"You are a helpful assistant called Llama-3. Write out your reasoning step-by-step to be sure you get the right answers!\"):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": query,\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=512,\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    )\n",
        "\n",
        "    print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Jg_bE_EfN9fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2ohEDE6bJ0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         system_prompt=\"You are a helpful assistant called Llama-3. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
        "         )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_kCSfG0bJ3g",
        "outputId": "d40625bf-7df8-43b6-c558-94684c442061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find out how much Weng earned, I'll follow these steps:\n",
            "\n",
            "1. Convert the babysitting time from minutes to hours:\n",
            "50 minutes ÷ 60 minutes per hour = 0.83 hours (I'll round this to two decimal places)\n",
            "\n",
            "2. Now that I have the time in hours, I'll multiply it by Weng's hourly wage:\n",
            "0.83 hours × $12 per hour = ?\n",
            "\n",
            "To multiply a decimal by a whole number, I'll multiply the whole number by the decimal's parts:\n",
            "$12 × 0.83 = $12 × (0.8 + 0.03) \n",
            "= ($12 × 0.8) + ($12 × 0.03) \n",
            "= $9.60 + $0.36\n",
            "= $10.00\n",
            "\n",
            "Therefore, Weng earned $10.00 for 50 minutes of babysitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6eihU0xblvv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}